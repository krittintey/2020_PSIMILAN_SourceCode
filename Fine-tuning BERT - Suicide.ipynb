{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda:1\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.964062e+07</td>\n",
       "      <td>The end.</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.350528e+07</td>\n",
       "      <td>GOD OVER EVERYTHING.</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.350528e+07</td>\n",
       "      <td>I'm sorry.</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.350528e+07</td>\n",
       "      <td>God... please forgive me.</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.350528e+07</td>\n",
       "      <td>This day couldn't get any worse...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>9.334510e+17</td>\n",
       "      <td>endless pain in life,end it</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>9.368910e+17</td>\n",
       "      <td>lets ave a lot of fun</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>9.123420e+17</td>\n",
       "      <td>Nothing left in this world for me,i want to die</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>9.368910e+17</td>\n",
       "      <td>I am successful in life</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>9.368910e+17</td>\n",
       "      <td>End my life</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id                                          Content Sentiment\n",
       "0    4.964062e+07                                         The end.  Negative\n",
       "1    4.350528e+07                             GOD OVER EVERYTHING.  Negative\n",
       "2    4.350528e+07                                       I'm sorry.  Negative\n",
       "3    4.350528e+07                        God... please forgive me.  Negative\n",
       "4    4.350528e+07               This day couldn't get any worse...  Negative\n",
       "..            ...                                              ...       ...\n",
       "298  9.334510e+17                      endless pain in life,end it  Negative\n",
       "299  9.368910e+17                            lets ave a lot of fun  Positive\n",
       "300  9.123420e+17  Nothing left in this world for me,i want to die  Negative\n",
       "301  9.368910e+17                          I am successful in life  Positive\n",
       "302  9.368910e+17                                      End my life  Negative\n",
       "\n",
       "[303 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Dataset/Twitter_Suicide_Data_new.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Negative': 175, 'Positive': 128})\n"
     ]
    }
   ],
   "source": [
    "# 0: NEGATIVE\n",
    "# 4: POSITIVE\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(df['Sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>RT @RickRoss: Even if you ain't got no paper,b...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>RT @dracomallfoys: yeah well math made me want...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>Done with the stress</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4.310000e+17</td>\n",
       "      <td>RT @FeelinggsSuckk: I'm more depressed then ev...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>@GemBrodie I have liked Ryder for quite awhile...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>RT @narendramodi: Delighted to join the Centen...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4.310000e+17</td>\n",
       "      <td>RT @TheBucktList: Before I die, I want to http...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4.310000e+17</td>\n",
       "      <td>RT @Drakee_YMCMB: Telling someone who's depres...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>I wish I can die already, this life is horrible</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4.310000e+17</td>\n",
       "      <td>Fear is what stops you... courage is what keep...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id                                            Content Sentiment\n",
       "132  9.200000e+17  RT @RickRoss: Even if you ain't got no paper,b...  Positive\n",
       "202  9.200000e+17  RT @dracomallfoys: yeah well math made me want...  Negative\n",
       "196  9.200000e+17                               Done with the stress  Negative\n",
       "75   4.310000e+17  RT @FeelinggsSuckk: I'm more depressed then ev...  Negative\n",
       "176  9.200000e+17  @GemBrodie I have liked Ryder for quite awhile...  Positive\n",
       "..            ...                                                ...       ...\n",
       "188  9.200000e+17  RT @narendramodi: Delighted to join the Centen...  Positive\n",
       "71   4.310000e+17  RT @TheBucktList: Before I die, I want to http...  Negative\n",
       "106  4.310000e+17  RT @Drakee_YMCMB: Telling someone who's depres...  Negative\n",
       "270  9.200000e+17    I wish I can die already, this life is horrible  Negative\n",
       "102  4.310000e+17  Fear is what stops you... courage is what keep...  Positive\n",
       "\n",
       "[242 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>RT @therealMickeyT: Wasn't the performance we ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>Each day, each hour, each minute is just tortu...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2.377192e+09</td>\n",
       "      <td>@mattarihinoaras enjoy</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>I Wish My True Love Happy Valentine's Day</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4.310000e+17</td>\n",
       "      <td>i bet i wud be cutting my wrist now if i were ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>I apologize</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4.310000e+17</td>\n",
       "      <td>Depressed</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>9.123420e+17</td>\n",
       "      <td>Nothing left in this world for me,i want to die</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>9.190000e+17</td>\n",
       "      <td>I be asking god why do I keep loosing people i...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>RT @PrincessSGB: I'm pleased to announce that ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id                                            Content Sentiment\n",
       "179  9.200000e+17  RT @therealMickeyT: Wasn't the performance we ...  Positive\n",
       "228  9.200000e+17  Each day, each hour, each minute is just tortu...  Negative\n",
       "111  2.377192e+09                             @mattarihinoaras enjoy  Positive\n",
       "246  9.200000e+17          I Wish My True Love Happy Valentine's Day  Positive\n",
       "60   4.310000e+17  i bet i wud be cutting my wrist now if i were ...  Negative\n",
       "..            ...                                                ...       ...\n",
       "249  9.200000e+17                                        I apologize  Negative\n",
       "104  4.310000e+17                                          Depressed  Negative\n",
       "300  9.123420e+17    Nothing left in this world for me,i want to die  Negative\n",
       "193  9.190000e+17  I be asking god why do I keep loosing people i...  Negative\n",
       "184  9.200000e+17  RT @PrincessSGB: I'm pleased to announce that ...  Positive\n",
       "\n",
       "[61 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    text = re.sub(r'#(\\w+)', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    text = re.sub(r'&quot;', ' ', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train['Content'].values\n",
    "y = df_train['Sentiment'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = { 'Negative': 0,\n",
    "    'Positive': 1,\n",
    "}\n",
    "\n",
    "y_train_encode = [encoding[key] for key in y_train]\n",
    "y_test = [encoding[key] for key in df_test['Sentiment'].values]\n",
    "y_val_encode = [encoding[key] for key in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  RT @RickRoss: Even if you ain't got no paper,bring your motivation to the table.\n",
      "Processed:  RT Even if you ain't got no paper,bring your motivation to the table.\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0\n",
    "print('Original: ', X[0])\n",
    "print('Processed: ', text_preprocessing(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  71\n"
     ]
    }
   ],
   "source": [
    "# Concatenate train data and test data\n",
    "all_tweets = np.concatenate([df_train['Content'].values, df_test['Content'].values])\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  RT @RickRoss: Even if you ain't got no paper,bring your motivation to the table.\n",
      "Token IDs:  [101, 19387, 2130, 2065, 2017, 7110, 1005, 1056, 2288, 2053, 3259, 1010, 3288, 2115, 14354, 2000, 1996, 2795, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psimilan/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 71\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train_encode)\n",
    "val_labels = torch.tensor(y_val_encode)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65 µs, sys: 6 µs, total: 71 µs\n",
      "Wall time: 76.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=3):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=3, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        #print(logits)\n",
    "        #print(b_labels)\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |    6    |   0.639230   |     -      |     -     |   0.99   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.639230   |  0.511106  |   90.99   |   1.06   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |    6    |   0.410380   |     -      |     -     |   0.96   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.410380   |  0.308764  |   89.43   |   1.03   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |    6    |   0.132184   |     -      |     -     |   0.96   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.132184   |  0.181189  |   95.50   |   1.03   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |    6    |   0.044426   |     -      |     -     |   0.94   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.044426   |  0.145791  |   95.50   |   1.01   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |    6    |   0.023457   |     -      |     -     |   0.94   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.023457   |  0.185822  |   93.93   |   1.01   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |    6    |   0.019768   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   0.019768   |  0.165600  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |    6    |   0.012868   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   0.012868   |  0.149386  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |    6    |   0.013422   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   0.013422   |  0.143006  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |    6    |   0.011835   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   0.011835   |  0.144742  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |    6    |   0.008903   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   0.008903   |  0.146228  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  11    |    6    |   0.008190   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "  11    |    -    |   0.008190   |  0.145709  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  12    |    6    |   0.009545   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "  12    |    -    |   0.009545   |  0.145704  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  13    |    6    |   0.007374   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "  13    |    -    |   0.007374   |  0.145853  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  14    |    6    |   0.007136   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "  14    |    -    |   0.007136   |  0.146763  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  15    |    6    |   0.006872   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "  15    |    -    |   0.006872   |  0.147657  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  16    |    6    |   0.008290   |     -      |     -     |   0.95   \n",
      "----------------------------------------------------------------------\n",
      "  16    |    -    |   0.008290   |  0.148341  |   95.50   |   1.02   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  17    |    6    |   0.006543   |     -      |     -     |   0.97   \n",
      "----------------------------------------------------------------------\n",
      "  17    |    -    |   0.006543   |  0.150038  |   95.50   |   1.03   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  18    |    6    |   0.007979   |     -      |     -     |   1.08   \n",
      "----------------------------------------------------------------------\n",
      "  18    |    -    |   0.007979   |  0.150893  |   95.50   |   1.16   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  19    |    6    |   0.006366   |     -      |     -     |   1.17   \n",
      "----------------------------------------------------------------------\n",
      "  19    |    -    |   0.006366   |  0.151324  |   95.50   |   1.24   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20    |    6    |   0.006336   |     -      |     -     |   1.13   \n",
      "----------------------------------------------------------------------\n",
      "  20    |    -    |   0.006336   |  0.151367  |   95.50   |   1.20   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=20)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=20, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9817\n",
      "Accuracy: 95.92%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_val_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.395516   |     -      |     -     |   2.92   \n",
      "   1    |   40    |   0.076552   |     -      |     -     |   2.74   \n",
      "   1    |   60    |   0.041534   |     -      |     -     |   2.72   \n",
      "   1    |   80    |   0.056735   |     -      |     -     |   2.75   \n",
      "   1    |   100   |   0.047529   |     -      |     -     |   2.70   \n",
      "   1    |   120   |   0.029075   |     -      |     -     |   2.72   \n",
      "   1    |   140   |   0.017800   |     -      |     -     |   2.71   \n",
      "   1    |   160   |   0.021259   |     -      |     -     |   2.71   \n",
      "   1    |   180   |   0.023377   |     -      |     -     |   2.70   \n",
      "   1    |   200   |   0.035580   |     -      |     -     |   2.69   \n",
      "   1    |   220   |   0.025894   |     -      |     -     |   2.70   \n",
      "   1    |   240   |   0.041768   |     -      |     -     |   2.72   \n",
      "   1    |   260   |   0.014341   |     -      |     -     |   2.72   \n",
      "   1    |   280   |   0.011376   |     -      |     -     |   2.71   \n",
      "   1    |   300   |   0.016533   |     -      |     -     |   2.72   \n",
      "   1    |   320   |   0.004703   |     -      |     -     |   2.72   \n",
      "   1    |   340   |   0.020214   |     -      |     -     |   2.73   \n",
      "   1    |   360   |   0.048679   |     -      |     -     |   2.74   \n",
      "   1    |   380   |   0.049293   |     -      |     -     |   2.72   \n",
      "   1    |   400   |   0.020120   |     -      |     -     |   2.72   \n",
      "   1    |   420   |   0.032923   |     -      |     -     |   2.72   \n",
      "   1    |   440   |   0.036409   |     -      |     -     |   2.73   \n",
      "   1    |   460   |   0.024441   |     -      |     -     |   2.74   \n",
      "   1    |   480   |   0.047097   |     -      |     -     |   2.74   \n",
      "   1    |   500   |   0.036398   |     -      |     -     |   2.73   \n",
      "   1    |   520   |   0.021526   |     -      |     -     |   2.72   \n",
      "   1    |   540   |   0.016162   |     -      |     -     |   2.73   \n",
      "   1    |   560   |   0.007679   |     -      |     -     |   2.73   \n",
      "   1    |   580   |   0.020263   |     -      |     -     |   2.74   \n",
      "   1    |   600   |   0.001832   |     -      |     -     |   2.73   \n",
      "   1    |   620   |   0.004811   |     -      |     -     |   2.73   \n",
      "   1    |   640   |   0.009038   |     -      |     -     |   2.73   \n",
      "   1    |   660   |   0.013796   |     -      |     -     |   2.76   \n",
      "   1    |   680   |   0.034352   |     -      |     -     |   2.77   \n",
      "   1    |   700   |   0.004024   |     -      |     -     |   2.76   \n",
      "   1    |   720   |   0.012922   |     -      |     -     |   2.76   \n",
      "   1    |   740   |   0.020312   |     -      |     -     |   2.75   \n",
      "   1    |   760   |   0.025714   |     -      |     -     |   2.74   \n",
      "   1    |   780   |   0.030863   |     -      |     -     |   2.75   \n",
      "   1    |   800   |   0.020861   |     -      |     -     |   2.74   \n",
      "   1    |   820   |   0.001899   |     -      |     -     |   2.74   \n",
      "   1    |   840   |   0.008741   |     -      |     -     |   2.73   \n",
      "   1    |   860   |   0.009669   |     -      |     -     |   2.75   \n",
      "   1    |   880   |   0.002423   |     -      |     -     |   2.75   \n",
      "   1    |   900   |   0.010641   |     -      |     -     |   2.75   \n",
      "   1    |   920   |   0.019522   |     -      |     -     |   2.75   \n",
      "   1    |   940   |   0.041712   |     -      |     -     |   2.76   \n",
      "   1    |   960   |   0.029445   |     -      |     -     |   2.76   \n",
      "   1    |   968   |   0.001423   |     -      |     -     |   1.07   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.022635   |     -      |     -     |   2.88   \n",
      "   2    |   40    |   0.010394   |     -      |     -     |   2.73   \n",
      "   2    |   60    |   0.001368   |     -      |     -     |   2.76   \n",
      "   2    |   80    |   0.004448   |     -      |     -     |   2.73   \n",
      "   2    |   100   |   0.014046   |     -      |     -     |   2.74   \n",
      "   2    |   120   |   0.022355   |     -      |     -     |   2.75   \n",
      "   2    |   140   |   0.007386   |     -      |     -     |   2.75   \n",
      "   2    |   160   |   0.001190   |     -      |     -     |   2.73   \n",
      "   2    |   180   |   0.008168   |     -      |     -     |   2.74   \n",
      "   2    |   200   |   0.007849   |     -      |     -     |   2.75   \n",
      "   2    |   220   |   0.013681   |     -      |     -     |   2.75   \n",
      "   2    |   240   |   0.002247   |     -      |     -     |   2.74   \n",
      "   2    |   260   |   0.005043   |     -      |     -     |   2.75   \n",
      "   2    |   280   |   0.005906   |     -      |     -     |   2.74   \n",
      "   2    |   300   |   0.001395   |     -      |     -     |   2.73   \n",
      "   2    |   320   |   0.016739   |     -      |     -     |   2.72   \n",
      "   2    |   340   |   0.011128   |     -      |     -     |   2.76   \n",
      "   2    |   360   |   0.001008   |     -      |     -     |   2.77   \n",
      "   2    |   380   |   0.001785   |     -      |     -     |   2.76   \n",
      "   2    |   400   |   0.017156   |     -      |     -     |   2.78   \n",
      "   2    |   420   |   0.000829   |     -      |     -     |   2.76   \n",
      "   2    |   440   |   0.006234   |     -      |     -     |   2.75   \n",
      "   2    |   460   |   0.011082   |     -      |     -     |   2.76   \n",
      "   2    |   480   |   0.000804   |     -      |     -     |   2.76   \n",
      "   2    |   500   |   0.014691   |     -      |     -     |   2.76   \n",
      "   2    |   520   |   0.000900   |     -      |     -     |   2.76   \n",
      "   2    |   540   |   0.009648   |     -      |     -     |   2.78   \n",
      "   2    |   560   |   0.017215   |     -      |     -     |   2.77   \n",
      "   2    |   580   |   0.021357   |     -      |     -     |   2.78   \n",
      "   2    |   600   |   0.017723   |     -      |     -     |   2.78   \n",
      "   2    |   620   |   0.001437   |     -      |     -     |   2.77   \n",
      "   2    |   640   |   0.010750   |     -      |     -     |   2.76   \n",
      "   2    |   660   |   0.018236   |     -      |     -     |   2.75   \n",
      "   2    |   680   |   0.000928   |     -      |     -     |   2.72   \n",
      "   2    |   700   |   0.000849   |     -      |     -     |   2.72   \n",
      "   2    |   720   |   0.009824   |     -      |     -     |   2.72   \n",
      "   2    |   740   |   0.000966   |     -      |     -     |   2.72   \n",
      "   2    |   760   |   0.000865   |     -      |     -     |   2.76   \n",
      "   2    |   780   |   0.001749   |     -      |     -     |   2.82   \n",
      "   2    |   800   |   0.006811   |     -      |     -     |   2.83   \n",
      "   2    |   820   |   0.018041   |     -      |     -     |   2.85   \n",
      "   2    |   840   |   0.001026   |     -      |     -     |   2.83   \n",
      "   2    |   860   |   0.008036   |     -      |     -     |   2.81   \n",
      "   2    |   880   |   0.001089   |     -      |     -     |   2.81   \n",
      "   2    |   900   |   0.011780   |     -      |     -     |   2.86   \n",
      "   2    |   920   |   0.000943   |     -      |     -     |   2.79   \n",
      "   2    |   940   |   0.037164   |     -      |     -     |   2.76   \n",
      "   2    |   960   |   0.012237   |     -      |     -     |   2.76   \n",
      "   2    |   968   |   0.000756   |     -      |     -     |   1.08   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Maybe not use\n",
    "# Concatenate the train set and the validation set\n",
    "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "full_train_sampler = RandomSampler(full_train_data)\n",
    "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
    "\n",
    "# Train the Bert Classifier on the entire training data\n",
    "set_seed(42)\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, full_train_dataloader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>RT @therealMickeyT: Wasn't the performance we ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.190000e+17</td>\n",
       "      <td>RT @memetribute: when people ask why your twit...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>9.369240e+17</td>\n",
       "      <td>I Want To Die</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>9.365750e+17</td>\n",
       "      <td>I just have this sick feeling that you would b...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>9.200000e+17</td>\n",
       "      <td>sbux is exclusively playing rostam and beck i ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id                                            Content Sentiment\n",
       "179  9.200000e+17  RT @therealMickeyT: Wasn't the performance we ...  Positive\n",
       "9    9.190000e+17  RT @memetribute: when people ask why your twit...  Negative\n",
       "281  9.369240e+17                                      I Want To Die  Negative\n",
       "284  9.365750e+17  I just have this sick feeling that you would b...  Negative\n",
       "175  9.200000e+17  sbux is exclusively playing rostam and beck i ...  Positive"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psimilan/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(df_test['Content'].values)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets predicted non-negative:  19\n"
     ]
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Get predictions from the probabilities\n",
    "threshold = 0.9\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of tweets predicted non-negative: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Checking:  Counter({'Negative': 40, 'Positive': 21})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Balanced Checking: \", Counter(df_test['Sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9895695  0.00193594 0.9884644  0.9896241  0.00199475 0.003085\n",
      " 0.98962396 0.00169207 0.00171212 0.00205175 0.00183084 0.00191619\n",
      " 0.00169926 0.98957753 0.9897007  0.0017548  0.9896197  0.9892859\n",
      " 0.02562626 0.0018451  0.9895063  0.00177544 0.0017598  0.00179678\n",
      " 0.9896395  0.9897383  0.9896521  0.00179985 0.0018732  0.98923117\n",
      " 0.98951167 0.00180449 0.98971564 0.00365685 0.13033128 0.0670877\n",
      " 0.00181906 0.00181375 0.00174021 0.03290012 0.00195217 0.00394256\n",
      " 0.98973536 0.9892657  0.00171866 0.9897204  0.00184281 0.00189986\n",
      " 0.00176689 0.8771329  0.00170639 0.9896137  0.00172599 0.0065579\n",
      " 0.01062915 0.00171579 0.00256356 0.00199223 0.00177847 0.38327172\n",
      " 0.80556005]\n",
      "[1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "[1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(probs[:, 1])\n",
    "print(preds)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.72%\n",
      "F1 Score: 96.68\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        40\n",
      "           1       1.00      0.90      0.95        21\n",
      "\n",
      "    accuracy                           0.97        61\n",
      "   macro avg       0.98      0.95      0.96        61\n",
      "weighted avg       0.97      0.97      0.97        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds, average='weighted')\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy*100))\n",
    "print(\"F1 Score: %.2f\" % (f1*100))\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40,  0],\n",
       "       [ 2, 19]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model/Fine-Tuning_BERT-Suicide.h5', 'wb') as model:\n",
    "    pickle.dump(bert_classifier, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
